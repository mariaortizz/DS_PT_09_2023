{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb01ca96934e5c83a36a2308da9645b87a9c52a0"
   },
   "source": [
    "## <center> Assignment 4. Sarcasm detection with logistic regression\n",
    "    \n",
    "We'll be using the dataset from the [paper](https://arxiv.org/abs/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. A processed version can be found on Kaggle in a form of a [Kaggle Dataset](https://www.kaggle.com/danofer/sarcasm).\n",
    "\n",
    "Sarcasm detection is easy. \n",
    "\n",
    "<img src=\"https://habrastorage.org/webt/1f/0d/ta/1f0dtavsd14ncf17gbsy1cvoga4.jpeg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de sarcasmo\n",
    "Para este ejercicio vas a utilizar el dataset del paper [A Large Self-Annotated Corpus for Sarcasm](https://arxiv.org/abs/1704.05579), con un millón de comentarios de Reddit, etiquetados como sarcásticos o no sarcásticos. Se pide:\n",
    "1. Si hubiese missings, eliminalos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "ffa03aec57ab6150f9bec0fa56cd3a5791a3e6f4"
   },
   "outputs": [],
   "source": [
    "# some necessary imports\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#modelo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  \\\n",
       "0      0                                         NC and NH.  Trumpbart   \n",
       "1      0  You do know west teams play against west teams...  Shbshb906   \n",
       "2      0  They were underdogs earlier today, but since G...   Creepeth   \n",
       "3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n",
       "4      0                    I could use one of those tools.  cush2push   \n",
       "\n",
       "            subreddit  score  ups  downs     date          created_utc  \\\n",
       "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
       "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
       "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
       "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
       "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                            They're favored to win.  \n",
       "3                         deadass don't kill my buzz  \n",
       "4  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../dataNLP/train-balanced-sarcasm.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tareas:\n",
    "1. Analizar el conjunto de datos, realizar algunos gráficos. Este [Kernel](https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc) podría servir como ejemplo.\n",
    "2. Construir un pipeline de Tf-Idf + regresión logística para predecir sarcasmo (`label`) basado en el texto de un comentario en Reddit (`comment`).\n",
    "3. Graficar las palabras/bigramas que son más predictivos de sarcasmo (puedes usar [eli5](https://github.com/TeamHG-Memex/eli5) para eso).\n",
    "4. (Opcionalmente) añadir subreddits como nuevas características para mejorar el rendimiento del modelo. Aplica aquí el enfoque de Bolsa de Palabras, es decir, trata cada subreddit como una nueva característica.\n",
    "\n",
    "## Enlaces:\n",
    "- Biblioteca de aprendizaje automático [Scikit-learn](https://scikit-learn.org/stable/index.html) (también conocida como sklearn).\n",
    "- Kernels sobre [regresión logística](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-2-classification) y sus aplicaciones a la [clasificación de texto](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-4-more-of-logit), también un [Kernel](https://www.kaggle.com/kashnitsky/topic-6-feature-engineering-and-feature-selection) sobre ingeniería de características y selección de características.\n",
    "- [Kernel de Kaggle](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle) \"Aproximándose a (Casi) Cualquier Problema de Procesamiento de Lenguaje Natural en Kaggle\".\n",
    "- [ELI5](https://github.com/TeamHG-Memex/eli5) para explicar las predicciones del modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primera exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1010826 entries, 0 to 1010825\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count    Dtype \n",
      "---  ------          --------------    ----- \n",
      " 0   label           1010826 non-null  int64 \n",
      " 1   comment         1010771 non-null  object\n",
      " 2   author          1010826 non-null  object\n",
      " 3   subreddit       1010826 non-null  object\n",
      " 4   score           1010826 non-null  int64 \n",
      " 5   ups             1010826 non-null  int64 \n",
      " 6   downs           1010826 non-null  int64 \n",
      " 7   date            1010826 non-null  object\n",
      " 8   created_utc     1010826 non-null  object\n",
      " 9   parent_comment  1010826 non-null  object\n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 77.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos los nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos los duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset='comment', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 962293 entries, 0 to 1010825\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   label           962293 non-null  int64 \n",
      " 1   comment         962293 non-null  object\n",
      " 2   author          962293 non-null  object\n",
      " 3   subreddit       962293 non-null  object\n",
      " 4   score           962293 non-null  int64 \n",
      " 5   ups             962293 non-null  int64 \n",
      " 6   downs           962293 non-null  int64 \n",
      " 7   date            962293 non-null  object\n",
      " 8   created_utc     962293 non-null  object\n",
      " 9   parent_comment  962293 non-null  object\n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 80.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                           NC and NH.\n",
       "1    You do know west teams play against west teams...\n",
       "2    They were underdogs earlier today, but since G...\n",
       "3    This meme isn't funny none of the \"new york ni...\n",
       "4                      I could use one of those tools.\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comment'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanceo dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el data set esta bastante balanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    484099\n",
       "0    478194\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='label', ylabel='percent'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df, x = 'label', stat='percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_W = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# import pandas as pd\n",
    "# import plotly.graph_objs as go\n",
    "# from plotly.subplots import make_subplots\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import plotly.tools as tools\n",
    "# import plotly.io as py\n",
    "\n",
    "# df1 = df[df[\"label\"]==1]\n",
    "# df0 = df[df[\"label\"]==0]\n",
    "\n",
    "# ## custom function for ngram generation ##\n",
    "# def generate_ngrams(text, n_gram=1):\n",
    "#     token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOP_W]\n",
    "#     ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "#     return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "# ## custom function for horizontal bar chart ##\n",
    "# def horizontal_bar_chart(df, color):\n",
    "#     trace = go.Bar(\n",
    "#         y=df[\"word\"].values[::-1],\n",
    "#         x=df[\"wordcount\"].values[::-1],\n",
    "#         showlegend=False,\n",
    "#         orientation = 'h',\n",
    "#         marker=dict(\n",
    "#             color=color,\n",
    "#         ),\n",
    "#     )\n",
    "#     return trace\n",
    "\n",
    "# ## Get the bar chart from sincere questions ##\n",
    "# freq_dict = defaultdict(int)\n",
    "# for sent in df0[\"comment\"]:\n",
    "#     for word in generate_ngrams(sent):\n",
    "#         freq_dict[word] += 1\n",
    "# fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "# fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "# trace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n",
    "\n",
    "# ## Get the bar chart from insincere questions ##\n",
    "# freq_dict = defaultdict(int)\n",
    "# for sent in df1[\"comment\"]:\n",
    "#     for word in generate_ngrams(sent):\n",
    "#         freq_dict[word] += 1\n",
    "# fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "# fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "# trace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n",
    "\n",
    "# # Creating two subplots\n",
    "# fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n",
    "#                           subplot_titles=[\"Frequent words of sincere questions\", \n",
    "#                                           \"Frequent words of insincere questions\"])\n",
    "# fig.append_trace(trace0, 1, 1)\n",
    "# fig.append_trace(trace1, 1, 2)\n",
    "# fig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n",
    "\n",
    "# plt.figure(figsize=(10,16))\n",
    "# sns.barplot(x=\"ngram_count\", y=\"ngram\", data=fd_sorted.loc[:50,:], color=\"b\")\n",
    "# plt.title(\"Frequent words for Insincere Questions\", fontsize=16)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['comment']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(769834,)\n",
      "(769834,)\n",
      "--------------------\n",
      "(192459,)\n",
      "(192459,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('-'*20)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamiento train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos los signos de puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595501    because banning movies has always been so succ...\n",
       "723461                                what is odd about you\n",
       "764551         why dont you just man up and pay for my kids\n",
       "428869                                        retirementduh\n",
       "763655                                      how commendable\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signos = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\¿)|(\\@)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)|(\\*)|(\\')|(-)|(/)\")\n",
    "\n",
    "def quitar_signos(x):\n",
    "    return signos.sub('', x.lower())\n",
    "\n",
    "X_train = X_train.apply(quitar_signos)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos un stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595501    becaus ban movi has alway been so success at q...\n",
       "723461                                what is odd about you\n",
       "764551          whi dont you just man up and pay for my kid\n",
       "428869                                        retirementduh\n",
       "763655                                          how commend\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def eng_stemmer(x):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    return \" \".join([stemmer.stem(word) for word in x.split()])\n",
    "\n",
    "X_train = X_train.apply(eng_stemmer)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizamos el train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', LogisticRegression())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__stop_words': [STOP_W]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline,\n",
    "                          parameters,\n",
    "                          cv = 5,\n",
    "                          n_jobs = -1,\n",
    "                          scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mega Tecnologia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning:\n",
      "\n",
      "\n",
      "2 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Mega Tecnologia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Mega Tecnologia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 402, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mega Tecnologia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 360, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mega Tecnologia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mega Tecnologia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 894, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mega Tecnologia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2121, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mega Tecnologia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1390, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mega Tecnologia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1239, in _limit_features\n",
      "    removed_terms.add(term)\n",
      "MemoryError\n",
      "\n",
      "\n",
      "c:\\Users\\Mega Tecnologia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning:\n",
      "\n",
      "One or more of the test scores are non-finite: [nan]\n",
      "\n",
      "c:\\Users\\Mega Tecnologia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                        TfidfVectorizer(max_features=10000,\n",
       "                                                        ngram_range=(1, 3))),\n",
       "                                       (&#x27;cls&#x27;, LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;vect__stop_words&#x27;: [[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                               &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n",
       "                                               &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;,\n",
       "                                               &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n",
       "                                               &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
       "                                               &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;,\n",
       "                                               &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;, &#x27;her&#x27;,\n",
       "                                               &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;,\n",
       "                                               &#x27;its&#x27;, &#x27;itself&#x27;, ...]]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                        TfidfVectorizer(max_features=10000,\n",
       "                                                        ngram_range=(1, 3))),\n",
       "                                       (&#x27;cls&#x27;, LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;vect__stop_words&#x27;: [[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                               &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n",
       "                                               &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;,\n",
       "                                               &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n",
       "                                               &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
       "                                               &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;,\n",
       "                                               &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;, &#x27;her&#x27;,\n",
       "                                               &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;,\n",
       "                                               &#x27;its&#x27;, &#x27;itself&#x27;, ...]]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 TfidfVectorizer(max_features=10000, ngram_range=(1, 3))),\n",
       "                (&#x27;cls&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=10000, ngram_range=(1, 3))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(max_features=10000,\n",
       "                                                        ngram_range=(1, 3))),\n",
       "                                       ('cls', LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                               'our', 'ours', 'ourselves',\n",
       "                                               'you', \"you're\", \"you've\",\n",
       "                                               \"you'll\", \"you'd\", 'your',\n",
       "                                               'yours', 'yourself',\n",
       "                                               'yourselves', 'he', 'him', 'his',\n",
       "                                               'himself', 'she', \"she's\", 'her',\n",
       "                                               'hers', 'herself', 'it', \"it's\",\n",
       "                                               'its', 'itself', ...]]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]}\n",
      "Best acc: nan\n",
      "Best model: Pipeline(steps=[('vect',\n",
      "                 TfidfVectorizer(max_features=10000, ngram_range=(1, 3),\n",
      "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
      "                                             'our', 'ours', 'ourselves', 'you',\n",
      "                                             \"you're\", \"you've\", \"you'll\",\n",
      "                                             \"you'd\", 'your', 'yours',\n",
      "                                             'yourself', 'yourselves', 'he',\n",
      "                                             'him', 'his', 'himself', 'she',\n",
      "                                             \"she's\", 'her', 'hers', 'herself',\n",
      "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
      "                ('cls', LogisticRegression())])\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best acc:\", grid_search.best_score_)\n",
    "print(\"Best model:\", grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicamos las tranformaciones al X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "903692    as long as we evenly downvote all of the rival...\n",
       "892225    check your professionalphotographytaking privi...\n",
       "677423    its not an assault weapon a cop is using it so...\n",
       "304332                             jeor was the original lc\n",
       "477813    when sanders starts to encourage acts of viole...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.apply(quitar_signos)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "903692    as long as we even downvot all of the rival co...\n",
       "892225       check your professionalphotographytak privileg\n",
       "677423    it not an assault weapon a cop is use it so it ok\n",
       "304332                               jeor was the origin lc\n",
       "477813    when sander start to encourag act of violenc f...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.apply(eng_stemmer)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacemos las predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predd = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6818179456403701"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, predd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[69214, 26590],\n",
       "       [34647, 62008]], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.72      0.69     95804\n",
      "           1       0.70      0.64      0.67     96655\n",
      "\n",
      "    accuracy                           0.68    192459\n",
      "   macro avg       0.68      0.68      0.68    192459\n",
      "weighted avg       0.68      0.68      0.68    192459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "            \n",
       "                \n",
       "                \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=1\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +5.977\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x4969\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 82.55%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +4.919\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x4924\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 83.27%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +4.631\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x3025\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 83.42%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +4.570\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x922\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 84.31%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +4.225\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x1268\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 84.57%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +4.126\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x1712\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 84.60%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +4.114\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x3678\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 84.77%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +4.051\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x1007\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 84.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +4.025\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x323\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 85.21%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +3.882\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x4428\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 85.27%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +3.859\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x321\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 85.28%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +3.858\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x749\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 85.66%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +3.718\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x2144\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 85.72%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +3.693\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x1835\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 85.75%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +3.684\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x4537\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 85.78%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +3.672\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x4923\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 85.95%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +3.608\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x1417\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.05%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +3.571\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x3893\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.06%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +3.570\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x476\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.06%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 2905 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 85.66%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 2076 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 85.66%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -3.715\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x2113\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "            \n",
       "        \n",
       "\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eli5\n",
    "eli5.show_weights(best_model.named_steps['cls'], \n",
    "                  vec=best_model.named_steps['vect'], \n",
    "                  top=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
